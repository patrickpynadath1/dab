from mucoco.losses import BaseLoss, register_loss


import torch 
import torch.nn.functional as F

def squared_cdist(x, E):
    # |x_i - y_j|_2^2 = <x_i - y_j, x_i - y_j> = <x_i, x_i> + <y_j, y_j> - 2*<x_i, y_j>
    x_sq_norm = x.square().sum(dim=-1, keepdim=True)
    y_sq_norm = E.t().square().sum(dim=0, keepdim=True)
    x_dot_y = x.bmm(E.t().unsqueeze(0))
    sq_dist = (x_sq_norm + y_sq_norm - 2*x_dot_y).clamp_(min=0.0)
    # print(sq_dist.size())
    return sq_dist
    # For numerical issues
    # sq_dist.clamp_(min=0.0)
    # return torch.sqrt(sq_dist)

@register_loss("uniquel22")
class UniqueNessLoss(BaseLoss):

    def __init__(self, model, tokenizer, args):
        super().__init__() 

        self.model = model 
        self.tokenizer = tokenizer 
        self.args = args
        self.device = model.device

        self.bos_token_id = self.tokenizer.bos_token_id
        self.eos_token_id = self.tokenizer.eos_token_id    

        self.C = (torch.tril(torch.ones((args.max_output_length, args.max_output_length))) - torch.diag(torch.ones(args.max_output_length))).to(self.device).detach()
    
    def compute_loss(self, batch, preds, **kwargs):
        '''
        batch: a tuple (source, prefix). If giving a prompt to the decoder, it can be specified using "prefix"
        preds: a tuple containing (predicted tokens, predicted embeddings, predicted probabilities), this is obtained through a forward pass on the optimizable target parameters (See utils/target.py)
        '''
        prompt, prefix = batch #prompt is the real deal, prefix can be provided as an extended prompt (generated by the model autoregressively)
        pred_tokens, pred_embeds, pred_probs = preds
        pred_probs = pred_probs[0]
        batch_size = prompt.size(0)

        embed_lut = self.model.get_input_embeddings()
        # print(prefix.size(), pred_tokens.size())
        input_tokens = torch.cat([prefix, pred_tokens], dim=1)
        input_embeds = torch.cat([embed_lut(prefix), pred_embeds], dim=1)

        C = self.C[:input_embeds.size(1), :input_embeds.size(1)]

        # input_probs = torch.softmax(-squared_cdist(input_embeds, embed_lut.weight), dim=-1)
        input_logprobs = torch.log_softmax(-torch.cdist(input_embeds, embed_lut.weight).square(), dim=-1)

        ull = []
        for b in range(batch_size):
            ull.append(input_logprobs[b].index_select(dim=-1, index=input_tokens[b]))
        unll = -torch.stack(ull, dim=0)
        # unll = torch.log(ul) #* C.unsqueeze(0)
        
        unll_q = (torch.exp(unll) * C.unsqueeze(0)) / ((torch.exp(unll) * C.unsqueeze(0)).sum(dim=-1, keepdims=True) + 1e-8)
        unll = (unll * unll_q).sum(dim=-1)
        
        unll = unll[:, 1:]
        unll_qq = F.softmax(unll, dim=-1)
        loss = (unll_qq * unll).sum(dim=-1)

        logging_output = {
            "loss": loss.data.cpu()
        }
        return loss, logging_output

    def compute_gold_loss(self, batch, **kwargs):
        '''
        given a discrete target output, this will compute the loss wrt to it. Useful in debugging
        '''
        with torch.no_grad():
            prompt, target = batch
            batch_size = prompt.size(0)

            embed_lut = self.model.get_input_embeddings()
            input_embeds = embed_lut(target)
            input_tokens = target

            # input_probs = torch.softmax(-squared_cdist(input_embeds, embed_lut.weight), dim=-1)
            input_probs = torch.softmax(-torch.cdist(input_embeds, embed_lut.weight).square(), dim=-1)
            C = self.C[:input_embeds.size(1), :input_embeds.size(1)]

            ul = []
            for b in range(batch_size):
                ul.append(input_probs.index_select(dim=-1, index=input_tokens[b]))
            ul = torch.cat(ul, dim=0)

            unll = torch.log(ul) #* C.unsqueeze(0)
            unll_q = (ul * C.unsqueeze(0)) /((ul * C.unsqueeze(0)).sum(dim=-1, keepdims=True) + 1e-8)
            
            unll = unll * unll_q
            
            unll = unll.sum(dim=-1)
            # print(unll)
            
            unll = unll[:, 1:]
            
            tau = 1.0
            unll_qq = F.softmax(unll/tau, dim=-1)
            
            loss = (unll_qq * unll).sum(dim=-1)
            logging_output = {
                "loss": loss.data.cpu()
            }

            return loss, logging_output   
    
